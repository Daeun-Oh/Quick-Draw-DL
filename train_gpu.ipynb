{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bae2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\desktop\\quick-draw-dl\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\quick-draw-dl\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6bd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: cuda\n",
      "\n",
      "ğŸ“¦ data0.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7238, Train Acc: 0.5485 | Val Loss: 1.2526, Val Acc: 0.6622\n",
      "Epoch 002 | Train Loss: 1.3466, Train Acc: 0.6353 | Val Loss: 1.1963, Val Acc: 0.6802\n",
      "Epoch 003 | Train Loss: 1.2701, Train Acc: 0.6509 | Val Loss: 1.1447, Val Acc: 0.6899\n",
      "Epoch 004 | Train Loss: 1.2190, Train Acc: 0.6652 | Val Loss: 1.1309, Val Acc: 0.6993\n",
      "Epoch 005 | Train Loss: 1.1814, Train Acc: 0.6750 | Val Loss: 1.1184, Val Acc: 0.7043\n",
      "Epoch 006 | Train Loss: 1.1606, Train Acc: 0.6775 | Val Loss: 1.1057, Val Acc: 0.7101\n",
      "Epoch 007 | Train Loss: 1.1378, Train Acc: 0.6842 | Val Loss: 1.0979, Val Acc: 0.7125\n",
      "Epoch 008 | Train Loss: 1.1226, Train Acc: 0.6882 | Val Loss: 1.1238, Val Acc: 0.7101\n",
      "Epoch 009 | Train Loss: 1.1079, Train Acc: 0.6911 | Val Loss: 1.0980, Val Acc: 0.7121\n",
      "Epoch 010 | Train Loss: 1.1002, Train Acc: 0.6907 | Val Loss: 1.1127, Val Acc: 0.7109\n",
      "Epoch 011 | Train Loss: 1.0853, Train Acc: 0.6963 | Val Loss: 1.1253, Val Acc: 0.7077\n",
      "Epoch 012 | Train Loss: 1.0764, Train Acc: 0.6972 | Val Loss: 1.1003, Val Acc: 0.7097\n",
      "Epoch 013 | Train Loss: 1.0689, Train Acc: 0.6988 | Val Loss: 1.1298, Val Acc: 0.7043\n",
      "Epoch 014 | Train Loss: 1.0620, Train Acc: 0.6990 | Val Loss: 1.1197, Val Acc: 0.7096\n",
      "Epoch 015 | Train Loss: 1.0582, Train Acc: 0.7021 | Val Loss: 1.1076, Val Acc: 0.7094\n",
      "Epoch 016 | Train Loss: 1.0516, Train Acc: 0.7026 | Val Loss: 1.1118, Val Acc: 0.7094\n",
      "Epoch 017 | Train Loss: 1.0478, Train Acc: 0.7070 | Val Loss: 1.1016, Val Acc: 0.7157\n",
      "Epoch 018 | Train Loss: 1.0327, Train Acc: 0.7076 | Val Loss: 1.1306, Val Acc: 0.7053\n",
      "Epoch 019 | Train Loss: 1.0268, Train Acc: 0.7084 | Val Loss: 1.1332, Val Acc: 0.7039\n",
      "Epoch 020 | Train Loss: 1.0195, Train Acc: 0.7104 | Val Loss: 1.1268, Val Acc: 0.7111\n",
      "Epoch 021 | Train Loss: 1.0140, Train Acc: 0.7108 | Val Loss: 1.1145, Val Acc: 0.7142\n",
      "Epoch 022 | Train Loss: 1.0155, Train Acc: 0.7122 | Val Loss: 1.1178, Val Acc: 0.7133\n",
      "Epoch 023 | Train Loss: 1.0114, Train Acc: 0.7112 | Val Loss: 1.1239, Val Acc: 0.7150\n",
      "Epoch 024 | Train Loss: 1.0068, Train Acc: 0.7141 | Val Loss: 1.1333, Val Acc: 0.7164\n",
      "Epoch 025 | Train Loss: 1.0034, Train Acc: 0.7151 | Val Loss: 1.1264, Val Acc: 0.7087\n",
      "Epoch 026 | Train Loss: 0.9955, Train Acc: 0.7166 | Val Loss: 1.1233, Val Acc: 0.7084\n",
      "Epoch 027 | Train Loss: 0.9938, Train Acc: 0.7146 | Val Loss: 1.1283, Val Acc: 0.7115\n",
      "Epoch 028 | Train Loss: 0.9970, Train Acc: 0.7183 | Val Loss: 1.1341, Val Acc: 0.7121\n",
      "Epoch 029 | Train Loss: 0.9878, Train Acc: 0.7174 | Val Loss: 1.1413, Val Acc: 0.7121\n",
      "Epoch 030 | Train Loss: 0.9880, Train Acc: 0.7169 | Val Loss: 1.1325, Val Acc: 0.7118\n",
      "Epoch 031 | Train Loss: 0.9891, Train Acc: 0.7181 | Val Loss: 1.1273, Val Acc: 0.7126\n",
      "Epoch 032 | Train Loss: 0.9806, Train Acc: 0.7192 | Val Loss: 1.1201, Val Acc: 0.7108\n",
      "Epoch 033 | Train Loss: 0.9833, Train Acc: 0.7203 | Val Loss: 1.1385, Val Acc: 0.7121\n",
      "Epoch 034 | Train Loss: 0.9827, Train Acc: 0.7201 | Val Loss: 1.1379, Val Acc: 0.7111\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7332, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.0439\n",
      "\n",
      "ğŸ“¦ data1.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7201, Train Acc: 0.5496 | Val Loss: 1.2158, Val Acc: 0.6821\n",
      "Epoch 002 | Train Loss: 1.3467, Train Acc: 0.6374 | Val Loss: 1.1399, Val Acc: 0.6985\n",
      "Epoch 003 | Train Loss: 1.2636, Train Acc: 0.6558 | Val Loss: 1.1045, Val Acc: 0.7085\n",
      "Epoch 004 | Train Loss: 1.2092, Train Acc: 0.6712 | Val Loss: 1.1175, Val Acc: 0.7074\n",
      "Epoch 005 | Train Loss: 1.1770, Train Acc: 0.6804 | Val Loss: 1.0820, Val Acc: 0.7198\n",
      "Epoch 006 | Train Loss: 1.1509, Train Acc: 0.6828 | Val Loss: 1.0559, Val Acc: 0.7221\n",
      "Epoch 007 | Train Loss: 1.1365, Train Acc: 0.6863 | Val Loss: 1.0673, Val Acc: 0.7231\n",
      "Epoch 008 | Train Loss: 1.1147, Train Acc: 0.6899 | Val Loss: 1.1039, Val Acc: 0.7135\n",
      "Epoch 009 | Train Loss: 1.1002, Train Acc: 0.6951 | Val Loss: 1.0689, Val Acc: 0.7243\n",
      "Epoch 010 | Train Loss: 1.0883, Train Acc: 0.6960 | Val Loss: 1.0693, Val Acc: 0.7231\n",
      "Epoch 011 | Train Loss: 1.0780, Train Acc: 0.6995 | Val Loss: 1.0701, Val Acc: 0.7226\n",
      "Epoch 012 | Train Loss: 1.0687, Train Acc: 0.7016 | Val Loss: 1.0605, Val Acc: 0.7315\n",
      "Epoch 013 | Train Loss: 1.0654, Train Acc: 0.7025 | Val Loss: 1.0631, Val Acc: 0.7296\n",
      "Epoch 014 | Train Loss: 1.0473, Train Acc: 0.7074 | Val Loss: 1.0772, Val Acc: 0.7308\n",
      "Epoch 015 | Train Loss: 1.0471, Train Acc: 0.7066 | Val Loss: 1.0793, Val Acc: 0.7294\n",
      "Epoch 016 | Train Loss: 1.0458, Train Acc: 0.7058 | Val Loss: 1.0782, Val Acc: 0.7291\n",
      "Epoch 017 | Train Loss: 1.0259, Train Acc: 0.7099 | Val Loss: 1.0818, Val Acc: 0.7284\n",
      "Epoch 018 | Train Loss: 1.0233, Train Acc: 0.7119 | Val Loss: 1.0741, Val Acc: 0.7265\n",
      "Epoch 019 | Train Loss: 1.0175, Train Acc: 0.7123 | Val Loss: 1.0802, Val Acc: 0.7303\n",
      "Epoch 020 | Train Loss: 1.0246, Train Acc: 0.7113 | Val Loss: 1.0692, Val Acc: 0.7304\n",
      "Epoch 021 | Train Loss: 1.0101, Train Acc: 0.7162 | Val Loss: 1.0632, Val Acc: 0.7340\n",
      "Epoch 022 | Train Loss: 1.0071, Train Acc: 0.7152 | Val Loss: 1.0740, Val Acc: 0.7303\n",
      "Epoch 023 | Train Loss: 0.9953, Train Acc: 0.7174 | Val Loss: 1.0798, Val Acc: 0.7287\n",
      "Epoch 024 | Train Loss: 0.9910, Train Acc: 0.7185 | Val Loss: 1.0896, Val Acc: 0.7234\n",
      "Epoch 025 | Train Loss: 0.9945, Train Acc: 0.7182 | Val Loss: 1.0898, Val Acc: 0.7303\n",
      "Epoch 026 | Train Loss: 0.9870, Train Acc: 0.7184 | Val Loss: 1.0908, Val Acc: 0.7291\n",
      "Epoch 027 | Train Loss: 0.9870, Train Acc: 0.7197 | Val Loss: 1.0833, Val Acc: 0.7342\n",
      "Epoch 028 | Train Loss: 0.9886, Train Acc: 0.7198 | Val Loss: 1.1032, Val Acc: 0.7260\n",
      "Epoch 029 | Train Loss: 0.9868, Train Acc: 0.7207 | Val Loss: 1.0892, Val Acc: 0.7265\n",
      "Epoch 030 | Train Loss: 0.9777, Train Acc: 0.7206 | Val Loss: 1.1058, Val Acc: 0.7250\n",
      "Epoch 031 | Train Loss: 0.9770, Train Acc: 0.7203 | Val Loss: 1.1011, Val Acc: 0.7244\n",
      "Epoch 032 | Train Loss: 0.9755, Train Acc: 0.7222 | Val Loss: 1.0881, Val Acc: 0.7304\n",
      "Epoch 033 | Train Loss: 0.9687, Train Acc: 0.7230 | Val Loss: 1.1027, Val Acc: 0.7277\n",
      "Epoch 034 | Train Loss: 0.9725, Train Acc: 0.7241 | Val Loss: 1.0944, Val Acc: 0.7306\n",
      "Epoch 035 | Train Loss: 0.9742, Train Acc: 0.7226 | Val Loss: 1.1051, Val Acc: 0.7248\n",
      "Epoch 036 | Train Loss: 0.9711, Train Acc: 0.7226 | Val Loss: 1.1035, Val Acc: 0.7272\n",
      "Epoch 037 | Train Loss: 0.9647, Train Acc: 0.7230 | Val Loss: 1.1004, Val Acc: 0.7280\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7272, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.1013\n",
      "\n",
      "ğŸ“¦ data2.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7357, Train Acc: 0.5441 | Val Loss: 1.2682, Val Acc: 0.6566\n",
      "Epoch 002 | Train Loss: 1.3550, Train Acc: 0.6344 | Val Loss: 1.1965, Val Acc: 0.6769\n",
      "Epoch 003 | Train Loss: 1.2702, Train Acc: 0.6528 | Val Loss: 1.1825, Val Acc: 0.6826\n",
      "Epoch 004 | Train Loss: 1.2293, Train Acc: 0.6647 | Val Loss: 1.1477, Val Acc: 0.6882\n",
      "Epoch 005 | Train Loss: 1.1899, Train Acc: 0.6740 | Val Loss: 1.1212, Val Acc: 0.7015\n",
      "Epoch 006 | Train Loss: 1.1694, Train Acc: 0.6764 | Val Loss: 1.1325, Val Acc: 0.6979\n",
      "Epoch 007 | Train Loss: 1.1413, Train Acc: 0.6838 | Val Loss: 1.1191, Val Acc: 0.7032\n",
      "Epoch 008 | Train Loss: 1.1245, Train Acc: 0.6876 | Val Loss: 1.1196, Val Acc: 0.7055\n",
      "Epoch 009 | Train Loss: 1.1141, Train Acc: 0.6903 | Val Loss: 1.1284, Val Acc: 0.7055\n",
      "Epoch 010 | Train Loss: 1.0961, Train Acc: 0.6942 | Val Loss: 1.1092, Val Acc: 0.7080\n",
      "Epoch 011 | Train Loss: 1.0849, Train Acc: 0.6985 | Val Loss: 1.1152, Val Acc: 0.7084\n",
      "Epoch 012 | Train Loss: 1.0777, Train Acc: 0.6996 | Val Loss: 1.1126, Val Acc: 0.7094\n",
      "Epoch 013 | Train Loss: 1.0706, Train Acc: 0.7010 | Val Loss: 1.1254, Val Acc: 0.7053\n",
      "Epoch 014 | Train Loss: 1.0623, Train Acc: 0.7020 | Val Loss: 1.1374, Val Acc: 0.7084\n",
      "Epoch 015 | Train Loss: 1.0471, Train Acc: 0.7038 | Val Loss: 1.1238, Val Acc: 0.7041\n",
      "Epoch 016 | Train Loss: 1.0454, Train Acc: 0.7059 | Val Loss: 1.1247, Val Acc: 0.7063\n",
      "Epoch 017 | Train Loss: 1.0370, Train Acc: 0.7099 | Val Loss: 1.1363, Val Acc: 0.7021\n",
      "Epoch 018 | Train Loss: 1.0394, Train Acc: 0.7091 | Val Loss: 1.1352, Val Acc: 0.7094\n",
      "Epoch 019 | Train Loss: 1.0302, Train Acc: 0.7105 | Val Loss: 1.1360, Val Acc: 0.7079\n",
      "Epoch 020 | Train Loss: 1.0273, Train Acc: 0.7096 | Val Loss: 1.1259, Val Acc: 0.7089\n",
      "Epoch 021 | Train Loss: 1.0207, Train Acc: 0.7125 | Val Loss: 1.1164, Val Acc: 0.7084\n",
      "Epoch 022 | Train Loss: 1.0148, Train Acc: 0.7130 | Val Loss: 1.1290, Val Acc: 0.7044\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7294, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.0432\n",
      "\n",
      "ğŸ“¦ data3.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7255, Train Acc: 0.5464 | Val Loss: 1.2385, Val Acc: 0.6745\n",
      "Epoch 002 | Train Loss: 1.3466, Train Acc: 0.6347 | Val Loss: 1.1588, Val Acc: 0.6925\n",
      "Epoch 003 | Train Loss: 1.2661, Train Acc: 0.6558 | Val Loss: 1.1392, Val Acc: 0.7029\n",
      "Epoch 004 | Train Loss: 1.2171, Train Acc: 0.6662 | Val Loss: 1.0889, Val Acc: 0.7178\n",
      "Epoch 005 | Train Loss: 1.1827, Train Acc: 0.6754 | Val Loss: 1.0897, Val Acc: 0.7138\n",
      "Epoch 006 | Train Loss: 1.1609, Train Acc: 0.6785 | Val Loss: 1.0796, Val Acc: 0.7212\n",
      "Epoch 007 | Train Loss: 1.1375, Train Acc: 0.6847 | Val Loss: 1.0737, Val Acc: 0.7142\n",
      "Epoch 008 | Train Loss: 1.1109, Train Acc: 0.6896 | Val Loss: 1.0738, Val Acc: 0.7246\n",
      "Epoch 009 | Train Loss: 1.1008, Train Acc: 0.6913 | Val Loss: 1.0851, Val Acc: 0.7202\n",
      "Epoch 010 | Train Loss: 1.0947, Train Acc: 0.6947 | Val Loss: 1.0735, Val Acc: 0.7227\n",
      "Epoch 011 | Train Loss: 1.0788, Train Acc: 0.6973 | Val Loss: 1.1018, Val Acc: 0.7183\n",
      "Epoch 012 | Train Loss: 1.0668, Train Acc: 0.7009 | Val Loss: 1.0798, Val Acc: 0.7239\n",
      "Epoch 013 | Train Loss: 1.0643, Train Acc: 0.7001 | Val Loss: 1.0761, Val Acc: 0.7239\n",
      "Epoch 014 | Train Loss: 1.0485, Train Acc: 0.7049 | Val Loss: 1.0840, Val Acc: 0.7209\n",
      "Epoch 015 | Train Loss: 1.0403, Train Acc: 0.7069 | Val Loss: 1.0933, Val Acc: 0.7215\n",
      "Epoch 016 | Train Loss: 1.0464, Train Acc: 0.7054 | Val Loss: 1.0762, Val Acc: 0.7232\n",
      "Epoch 017 | Train Loss: 1.0305, Train Acc: 0.7064 | Val Loss: 1.0751, Val Acc: 0.7268\n",
      "Epoch 018 | Train Loss: 1.0306, Train Acc: 0.7081 | Val Loss: 1.0778, Val Acc: 0.7243\n",
      "Epoch 019 | Train Loss: 1.0247, Train Acc: 0.7111 | Val Loss: 1.0963, Val Acc: 0.7166\n",
      "Epoch 020 | Train Loss: 1.0118, Train Acc: 0.7135 | Val Loss: 1.0793, Val Acc: 0.7282\n",
      "Epoch 021 | Train Loss: 1.0193, Train Acc: 0.7091 | Val Loss: 1.0712, Val Acc: 0.7232\n",
      "Epoch 022 | Train Loss: 1.0107, Train Acc: 0.7130 | Val Loss: 1.0680, Val Acc: 0.7251\n",
      "Epoch 023 | Train Loss: 0.9998, Train Acc: 0.7151 | Val Loss: 1.0858, Val Acc: 0.7214\n",
      "Epoch 024 | Train Loss: 0.9895, Train Acc: 0.7190 | Val Loss: 1.0998, Val Acc: 0.7197\n",
      "Epoch 025 | Train Loss: 1.0058, Train Acc: 0.7142 | Val Loss: 1.0726, Val Acc: 0.7304\n",
      "Epoch 026 | Train Loss: 0.9914, Train Acc: 0.7179 | Val Loss: 1.0748, Val Acc: 0.7244\n",
      "Epoch 027 | Train Loss: 0.9872, Train Acc: 0.7181 | Val Loss: 1.1063, Val Acc: 0.7243\n",
      "Epoch 028 | Train Loss: 0.9912, Train Acc: 0.7173 | Val Loss: 1.0714, Val Acc: 0.7282\n",
      "Epoch 029 | Train Loss: 0.9821, Train Acc: 0.7198 | Val Loss: 1.0730, Val Acc: 0.7274\n",
      "Epoch 030 | Train Loss: 0.9866, Train Acc: 0.7180 | Val Loss: 1.0761, Val Acc: 0.7294\n",
      "Epoch 031 | Train Loss: 0.9826, Train Acc: 0.7196 | Val Loss: 1.0845, Val Acc: 0.7270\n",
      "Epoch 032 | Train Loss: 0.9814, Train Acc: 0.7210 | Val Loss: 1.1033, Val Acc: 0.7268\n",
      "Epoch 033 | Train Loss: 0.9785, Train Acc: 0.7194 | Val Loss: 1.0779, Val Acc: 0.7292\n",
      "Epoch 034 | Train Loss: 0.9745, Train Acc: 0.7198 | Val Loss: 1.0905, Val Acc: 0.7250\n",
      "Epoch 035 | Train Loss: 0.9723, Train Acc: 0.7223 | Val Loss: 1.0932, Val Acc: 0.7231\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7378, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.0104\n",
      "\n",
      "ğŸ“¦ data4.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7319, Train Acc: 0.5447 | Val Loss: 1.2093, Val Acc: 0.6785\n",
      "Epoch 002 | Train Loss: 1.3604, Train Acc: 0.6365 | Val Loss: 1.0998, Val Acc: 0.7067\n",
      "Epoch 003 | Train Loss: 1.2707, Train Acc: 0.6567 | Val Loss: 1.0612, Val Acc: 0.7116\n",
      "Epoch 004 | Train Loss: 1.2222, Train Acc: 0.6664 | Val Loss: 1.0670, Val Acc: 0.7152\n",
      "Epoch 005 | Train Loss: 1.1873, Train Acc: 0.6738 | Val Loss: 1.0642, Val Acc: 0.7173\n",
      "Epoch 006 | Train Loss: 1.1601, Train Acc: 0.6796 | Val Loss: 1.0251, Val Acc: 0.7256\n",
      "Epoch 007 | Train Loss: 1.1432, Train Acc: 0.6845 | Val Loss: 1.0384, Val Acc: 0.7277\n",
      "Epoch 008 | Train Loss: 1.1267, Train Acc: 0.6881 | Val Loss: 1.0417, Val Acc: 0.7260\n",
      "Epoch 009 | Train Loss: 1.1164, Train Acc: 0.6918 | Val Loss: 1.0347, Val Acc: 0.7296\n",
      "Epoch 010 | Train Loss: 1.0989, Train Acc: 0.6945 | Val Loss: 1.0467, Val Acc: 0.7212\n",
      "Epoch 011 | Train Loss: 1.0898, Train Acc: 0.6961 | Val Loss: 1.0452, Val Acc: 0.7274\n",
      "Epoch 012 | Train Loss: 1.0793, Train Acc: 0.6972 | Val Loss: 1.0274, Val Acc: 0.7311\n",
      "Epoch 013 | Train Loss: 1.0647, Train Acc: 0.7012 | Val Loss: 1.0388, Val Acc: 0.7268\n",
      "Epoch 014 | Train Loss: 1.0572, Train Acc: 0.7030 | Val Loss: 1.0486, Val Acc: 0.7267\n",
      "Epoch 015 | Train Loss: 1.0612, Train Acc: 0.7019 | Val Loss: 1.0451, Val Acc: 0.7299\n",
      "Epoch 016 | Train Loss: 1.0567, Train Acc: 0.7031 | Val Loss: 1.0374, Val Acc: 0.7308\n",
      "Epoch 017 | Train Loss: 1.0429, Train Acc: 0.7064 | Val Loss: 1.0362, Val Acc: 0.7333\n",
      "Epoch 018 | Train Loss: 1.0374, Train Acc: 0.7085 | Val Loss: 1.0326, Val Acc: 0.7318\n",
      "Epoch 019 | Train Loss: 1.0324, Train Acc: 0.7101 | Val Loss: 1.0355, Val Acc: 0.7287\n",
      "Epoch 020 | Train Loss: 1.0287, Train Acc: 0.7094 | Val Loss: 1.0475, Val Acc: 0.7291\n",
      "Epoch 021 | Train Loss: 1.0217, Train Acc: 0.7118 | Val Loss: 1.0345, Val Acc: 0.7330\n",
      "Epoch 022 | Train Loss: 1.0161, Train Acc: 0.7124 | Val Loss: 1.0493, Val Acc: 0.7304\n",
      "Epoch 023 | Train Loss: 1.0180, Train Acc: 0.7133 | Val Loss: 1.0511, Val Acc: 0.7270\n",
      "Epoch 024 | Train Loss: 1.0093, Train Acc: 0.7140 | Val Loss: 1.0579, Val Acc: 0.7251\n",
      "Epoch 025 | Train Loss: 1.0096, Train Acc: 0.7138 | Val Loss: 1.0374, Val Acc: 0.7303\n",
      "Epoch 026 | Train Loss: 1.0106, Train Acc: 0.7134 | Val Loss: 1.0305, Val Acc: 0.7345\n",
      "Epoch 027 | Train Loss: 0.9884, Train Acc: 0.7214 | Val Loss: 1.0473, Val Acc: 0.7309\n",
      "Epoch 028 | Train Loss: 0.9978, Train Acc: 0.7163 | Val Loss: 1.0400, Val Acc: 0.7345\n",
      "Epoch 029 | Train Loss: 0.9955, Train Acc: 0.7175 | Val Loss: 1.0505, Val Acc: 0.7231\n",
      "Epoch 030 | Train Loss: 0.9967, Train Acc: 0.7169 | Val Loss: 1.0520, Val Acc: 0.7279\n",
      "Epoch 031 | Train Loss: 0.9948, Train Acc: 0.7184 | Val Loss: 1.0517, Val Acc: 0.7268\n",
      "Epoch 032 | Train Loss: 0.9881, Train Acc: 0.7170 | Val Loss: 1.0407, Val Acc: 0.7316\n",
      "Epoch 033 | Train Loss: 0.9828, Train Acc: 0.7205 | Val Loss: 1.0489, Val Acc: 0.7333\n",
      "Epoch 034 | Train Loss: 0.9858, Train Acc: 0.7181 | Val Loss: 1.0635, Val Acc: 0.7263\n",
      "Epoch 035 | Train Loss: 0.9818, Train Acc: 0.7217 | Val Loss: 1.0560, Val Acc: 0.7296\n",
      "Epoch 036 | Train Loss: 0.9768, Train Acc: 0.7212 | Val Loss: 1.0611, Val Acc: 0.7267\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7305, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.0565\n",
      "\n",
      "ğŸ“¦ data5.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7241, Train Acc: 0.5506 | Val Loss: 1.2179, Val Acc: 0.6779\n",
      "Epoch 002 | Train Loss: 1.3476, Train Acc: 0.6355 | Val Loss: 1.1352, Val Acc: 0.6974\n",
      "Epoch 003 | Train Loss: 1.2683, Train Acc: 0.6560 | Val Loss: 1.1042, Val Acc: 0.7104\n",
      "Epoch 004 | Train Loss: 1.2200, Train Acc: 0.6640 | Val Loss: 1.0875, Val Acc: 0.7125\n",
      "Epoch 005 | Train Loss: 1.1899, Train Acc: 0.6739 | Val Loss: 1.0690, Val Acc: 0.7202\n",
      "Epoch 006 | Train Loss: 1.1629, Train Acc: 0.6777 | Val Loss: 1.0710, Val Acc: 0.7200\n",
      "Epoch 007 | Train Loss: 1.1400, Train Acc: 0.6857 | Val Loss: 1.0804, Val Acc: 0.7176\n",
      "Epoch 008 | Train Loss: 1.1287, Train Acc: 0.6867 | Val Loss: 1.0505, Val Acc: 0.7270\n",
      "Epoch 009 | Train Loss: 1.1137, Train Acc: 0.6900 | Val Loss: 1.0634, Val Acc: 0.7272\n",
      "Epoch 010 | Train Loss: 1.0942, Train Acc: 0.6955 | Val Loss: 1.0794, Val Acc: 0.7185\n",
      "Epoch 011 | Train Loss: 1.0886, Train Acc: 0.6974 | Val Loss: 1.0645, Val Acc: 0.7279\n",
      "Epoch 012 | Train Loss: 1.0804, Train Acc: 0.6988 | Val Loss: 1.0623, Val Acc: 0.7234\n",
      "Epoch 013 | Train Loss: 1.0598, Train Acc: 0.7016 | Val Loss: 1.0599, Val Acc: 0.7256\n",
      "Epoch 014 | Train Loss: 1.0562, Train Acc: 0.7042 | Val Loss: 1.0611, Val Acc: 0.7325\n",
      "Epoch 015 | Train Loss: 1.0498, Train Acc: 0.7054 | Val Loss: 1.0554, Val Acc: 0.7258\n",
      "Epoch 016 | Train Loss: 1.0553, Train Acc: 0.7055 | Val Loss: 1.0474, Val Acc: 0.7316\n",
      "Epoch 017 | Train Loss: 1.0399, Train Acc: 0.7075 | Val Loss: 1.0596, Val Acc: 0.7296\n",
      "Epoch 018 | Train Loss: 1.0345, Train Acc: 0.7078 | Val Loss: 1.0449, Val Acc: 0.7231\n",
      "Epoch 019 | Train Loss: 1.0235, Train Acc: 0.7126 | Val Loss: 1.0484, Val Acc: 0.7303\n",
      "Epoch 020 | Train Loss: 1.0252, Train Acc: 0.7087 | Val Loss: 1.0659, Val Acc: 0.7289\n",
      "Epoch 021 | Train Loss: 1.0148, Train Acc: 0.7123 | Val Loss: 1.0644, Val Acc: 0.7255\n",
      "Epoch 022 | Train Loss: 1.0109, Train Acc: 0.7129 | Val Loss: 1.0687, Val Acc: 0.7277\n",
      "Epoch 023 | Train Loss: 1.0082, Train Acc: 0.7140 | Val Loss: 1.0881, Val Acc: 0.7219\n",
      "Epoch 024 | Train Loss: 1.0016, Train Acc: 0.7173 | Val Loss: 1.0702, Val Acc: 0.7246\n",
      "â¹ï¸ Early stopping\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.7229, í…ŒìŠ¤íŠ¸ ì†ì‹¤: 1.0878\n",
      "\n",
      "ğŸ“¦ data6.npy í•™ìŠµ ì‹œì‘...\n",
      "Epoch 001 | Train Loss: 1.7175, Train Acc: 0.5510 | Val Loss: 1.2208, Val Acc: 0.6713\n",
      "Epoch 002 | Train Loss: 1.3445, Train Acc: 0.6358 | Val Loss: 1.1377, Val Acc: 0.7005\n",
      "Epoch 003 | Train Loss: 1.2654, Train Acc: 0.6560 | Val Loss: 1.1024, Val Acc: 0.7094\n",
      "Epoch 004 | Train Loss: 1.2217, Train Acc: 0.6654 | Val Loss: 1.0929, Val Acc: 0.7149\n",
      "Epoch 005 | Train Loss: 1.1809, Train Acc: 0.6747 | Val Loss: 1.0768, Val Acc: 0.7221\n",
      "Epoch 006 | Train Loss: 1.1571, Train Acc: 0.6798 | Val Loss: 1.0531, Val Acc: 0.7270\n",
      "Epoch 007 | Train Loss: 1.1322, Train Acc: 0.6861 | Val Loss: 1.0674, Val Acc: 0.7219\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m    140\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m--> 141\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_val_acc:\n",
      "Cell \u001b[1;32mIn[10], line 68\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, criterion)\u001b[0m\n\u001b[0;32m     66\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     69\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mNpyImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     42\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 44\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Quick-Draw-DL\\.venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    919\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# âœ… ê¸°ë³¸ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤:\", device)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "patience = 10  # early stopping patience\n",
    "\n",
    "# âœ… ì¹´í…Œê³ ë¦¬ ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "categories = np.load(\"category.npy\").tolist()\n",
    "num_classes = len(categories)\n",
    "\n",
    "# âœ… ì „ì²˜ë¦¬ transform ì •ì˜\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "\n",
    "# âœ… ì»¤ìŠ¤í…€ Dataset ì •ì˜\n",
    "class NpyImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = int(self.labels[idx])\n",
    "        return img, label\n",
    "\n",
    "# âœ… ëª¨ë¸ ì •ì˜ (EfficientNet + ë¶„ë¥˜ê¸° êµì²´)\n",
    "def create_model():\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(model.classifier[1].in_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# âœ… í‰ê°€ í•¨ìˆ˜ (ì •í™•ë„ + ì†ì‹¤ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •)\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "# âœ… ì „ì²´ í•™ìŠµ ë£¨í”„ (49ê°œ ë°ì´í„°ì…‹ ë°˜ë³µ)\n",
    "for i in range(49):\n",
    "    print(f\"\\nğŸ“¦ data{i}.npy í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "    # ğŸ—‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    data_path = f\"data/data{i}.npy\"\n",
    "    target_path = f\"data/target{i}.npy\"\n",
    "    data = np.load(data_path)  # (N, H, W)\n",
    "    labels = np.load(target_path)\n",
    "\n",
    "    # ğŸ“ ë¦¬ì‚¬ì´ì§• (224x224) â†’ (N, 224, 224)\n",
    "    resized_data = np.zeros((data.shape[0], 224, 224), dtype=np.uint8)\n",
    "    for j in range(data.shape[0]):\n",
    "        resized_data[j] = np.array(Image.fromarray(data[j]).resize((224, 224)))\n",
    "\n",
    "    # ğŸ”€ train/val/test ë¶„í• \n",
    "    train_imgs, test_imgs, train_labels, test_labels = train_test_split(resized_data, labels, test_size=0.1, random_state=42)\n",
    "    train_imgs, val_imgs, train_labels, val_labels = train_test_split(train_imgs, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "    # ğŸ“¦ Dataset / DataLoader ì¤€ë¹„ (num_workers=0 ë°˜ë“œì‹œ!)\n",
    "    train_ds = NpyImageDataset(train_imgs, train_labels, transform)\n",
    "    val_ds   = NpyImageDataset(val_imgs, val_labels, transform)\n",
    "    test_ds  = NpyImageDataset(test_imgs, test_labels, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # ğŸ§  ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "    # ğŸ‹ï¸â€â™€ï¸ í•™ìŠµ\n",
    "    best_val_acc = 0\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        val_acc, val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best-model.pth\")\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"â¹ï¸ Early stopping\")\n",
    "                break\n",
    "\n",
    "    # âœ… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í™•ì¸\n",
    "    model.load_state_dict(torch.load(\"best-model.pth\"))\n",
    "    test_acc, test_loss = evaluate(model, test_loader, criterion)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}, í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
